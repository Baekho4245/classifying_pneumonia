{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directory names\n",
    "train_data_dir = 'chest_xray/train/'\n",
    "val_data_dir = 'chest_xray/val/'\n",
    "test_data_dir = 'chest_xray/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Get all the data in the directory data/train (5216 images), and reshape them into (64, 64)\n",
    "train_generator = ImageDataGenerator().flow_from_directory(\n",
    "        train_data_dir, \n",
    "        target_size=(64, 64), batch_size=5216)\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "        val_data_dir, \n",
    "        target_size=(64, 64), batch_size=16)\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "        test_data_dir, \n",
    "        target_size=(64, 64), batch_size=624)\n",
    "\n",
    "# Create the datasets\n",
    "train_images, train_labels = next(train_generator)\n",
    "val_images, val_labels = next(val_generator)\n",
    "test_images, test_labels = next(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5216, 64, 64, 3)\n",
      "(5216, 2)\n",
      "(16, 64, 64, 3)\n",
      "(16, 2)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_images))\n",
    "print(np.shape(train_labels))\n",
    "print(np.shape(val_images))\n",
    "print(np.shape(val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12288, 5216)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img_unrow = train_images.reshape(5216, -1).T\n",
    "val_img_unrow = val_images.reshape(16, -1).T\n",
    "test_img_unrow = test_images.reshape(16, -1).T\n",
    "np.shape(train_img_unrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NORMAL': 0, 'PNEUMONIA': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5216)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_final = train_labels.T[[1]]\n",
    "val_labels_final = val_labels.T[[1]]\n",
    "test_labels_final = test_labels.T[[1]]\n",
    "np.shape(train_labels_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12288, 5216)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img_final = (train_img_unrow/255).astype('float32')\n",
    "val_img_final = (val_img_unrow/255).astype('float32')\n",
    "test_img_final = (test_img_unrow/255).astype('float32')\n",
    "train_img_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empyty array with size N\n",
    "def init_w(n):\n",
    "    w = np.zeros((n, 1))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the cost from the current gradient\n",
    "def propagation(w, b, x, y):\n",
    "    l = x.shape[1]\n",
    "    y_hat = 1/(1 + np.exp(- (np.dot(w.T, x) + b)))                                  \n",
    "    cost = -(1/l) * np.sum(y * np.log(y_hat) + (1-y)* np.log(1 - y_hat))    \n",
    "    dw = (1/l) * np.dot(x,(y_hat - y).T)\n",
    "    db = (1/l) * np.sum(y_hat - y)\n",
    "    return dw, db, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the learning rate and repeatedly calls the propagation function to \"gradually\" decrease to the lowest cost\n",
    "def optimization(w, b, x, y, num_iterations, learning_rate, print_cost = False):\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        dw, db, cost = propagation(w, b, x, y)    \n",
    "        w = w - learning_rate*dw\n",
    "        b = b - learning_rate*db\n",
    "        \n",
    "        # Record the costs and print them every 50 iterations\n",
    "        if i % 50 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost and i % 50 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    return w, b, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes predictions in our data, if y_hat > 0.5, then image is classified as 'PNEUMONIA'\n",
    "def prediction(w, b, x):\n",
    "    l = x.shape[1]\n",
    "    y_prediction = np.zeros((1, l))\n",
    "    w = w.reshape(x.shape[0], 1)\n",
    "    y_hat = 1/(1 + np.exp(- (np.dot(w.T, x) + b))) \n",
    "    p = y_hat\n",
    "    \n",
    "    for i in range(y_hat.shape[1]):\n",
    "        if (y_hat[0,i] > 0.5): \n",
    "            y_prediction[0, i] = 1\n",
    "        else:\n",
    "            y_prediction[0, i] = 0\n",
    "    return y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combines the above functions to create a logistic regression model by entering train, test datasets, the number of \n",
    "# iterations and learning rate.\n",
    "def model(x_train, y_train, x_test, y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "\n",
    "    b = 0\n",
    "    w = init_w(np.shape(x_train)[0]) \n",
    "\n",
    "    # Gradient descent (â‰ˆ 1 line of code)\n",
    "    w, b, costs = optimization(w, b, x_train, y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    y_pred_test = prediction(w, b, x_test)\n",
    "    y_pred_train = prediction(w, b, x_train)\n",
    "\n",
    "    # Print train/test errors\n",
    "    print('train accuracy: {} %'.format(100 - np.mean(np.abs(y_pred_train - y_train)) * 100))\n",
    "    print('test accuracy: {} %'.format(100 - np.mean(np.abs(y_pred_test - y_test)) * 100))\n",
    "\n",
    "    output = {'costs': costs, \n",
    "              'y_pred_test': y_pred_test,  \n",
    "              'y_pred_train' : y_pred_train,  \n",
    "              'w' : w, \n",
    "              'b' : b, \n",
    "              'learning_rate' : learning_rate, \n",
    "              'num_iterations': num_iterations}\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 95.0536809815951 %\n",
      "test accuracy: 87.5 %\n"
     ]
    }
   ],
   "source": [
    "# Baseline - Baseline. 500 iterations at 0.01 learning rate\n",
    "output = model(train_img_final, train_labels_final, val_img_final, val_labels_final, \n",
    "               num_iterations=500, learning_rate=0.01, print_cost=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 50: 1.524936\n",
      "Cost after iteration 100: 0.447747\n",
      "Cost after iteration 150: 0.316015\n",
      "Cost after iteration 200: 0.188705\n",
      "Cost after iteration 250: 0.163196\n",
      "train accuracy: 94.1909509202454 %\n",
      "test accuracy: 87.5 %\n"
     ]
    }
   ],
   "source": [
    "# Possibly overfitting, a lower number of iterations - at 300 - underfits our model by a tad, but not much\n",
    "output = model(train_img_final, train_labels_final, val_img_final, val_labels_final, \n",
    "               num_iterations=300, learning_rate=0.01, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 50: 0.497045\n",
      "Cost after iteration 100: 0.326489\n",
      "Cost after iteration 150: 0.242337\n",
      "train accuracy: 92.4463190184049 %\n",
      "test accuracy: 81.25 %\n"
     ]
    }
   ],
   "source": [
    "# Decrease the learning rate while also lowering the number of iterations made our model fit not as much, but our metric \n",
    "# scores decreased drastically\n",
    "output = model(train_img_final, train_labels_final, val_img_final, val_labels_final, \n",
    "               num_iterations=200, learning_rate=0.005, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 50: 1.524936\n",
      "Cost after iteration 100: 0.447747\n",
      "Cost after iteration 150: 0.316015\n",
      "train accuracy: 92.48466257668711 %\n",
      "test accuracy: 87.5 %\n"
     ]
    }
   ],
   "source": [
    "# In order to regain out testing accuracy metric of 87.5%, we re-increased the learning rate to 0.01\n",
    "output = model(train_img_final, train_labels_final, val_img_final, val_labels_final, \n",
    "               num_iterations=200, learning_rate=0.01, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 50: 1.524936\n",
      "Cost after iteration 100: 0.447747\n",
      "Cost after iteration 150: 0.316015\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 12288 into shape (479232,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-14d9e51c486d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Score the best model on the testing data instead of the validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m output = model(train_img_final, train_labels_final, test_img_final, test_labels_final, \n\u001b[0m\u001b[0;32m      3\u001b[0m                num_iterations=200, learning_rate=0.01, print_cost=True)\n",
      "\u001b[1;32m<ipython-input-13-517c1da5bda5>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(x_train, y_train, x_test, y_test, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcosts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_cost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0my_pred_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0my_pred_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-9bf424a10d1e>\u001b[0m in \u001b[0;36mprediction\u001b[1;34m(w, b, x)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0my_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 12288 into shape (479232,1)"
     ]
    }
   ],
   "source": [
    "# Score the best model on the testing data instead of the validation data\n",
    "output = model(train_img_final, train_labels_final, test_img_final, test_labels_final, \n",
    "               num_iterations=200, learning_rate=0.01, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_mlp = (train_images / 255).astype('float32')\n",
    "val_images_mlp = (val_images / 255).astype('float32')\n",
    "test_images_mlp = (test_images / 255).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = models.Sequential()\n",
    "model_1.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        strides=(1,1),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(64, 64, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model_1.add(layers.MaxPooling2D(pool_size=(2,2),\n",
    "                     strides=2))\n",
    "model_1.add(layers.Flatten())        \n",
    "model_1.add(layers.Dense(128))\n",
    "model_1.add(layers.Activation('relu'))\n",
    "#model_1.add(layers.Dropout(0.25))\n",
    "model_1.add(layers.Dense(2))\n",
    "model_1.add(layers.Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 64, 64, 32)        416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               4194432   \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 4,195,106\n",
      "Trainable params: 4,195,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "105/105 [==============================] - 4s 43ms/step - loss: 0.4333 - accuracy: 0.8273\n",
      "Epoch 2/5\n",
      "105/105 [==============================] - 4s 41ms/step - loss: 0.1521 - accuracy: 0.9448\n",
      "Epoch 3/5\n",
      "105/105 [==============================] - 4s 41ms/step - loss: 0.1135 - accuracy: 0.9571\n",
      "Epoch 4/5\n",
      "105/105 [==============================] - 4s 41ms/step - loss: 0.0901 - accuracy: 0.9666\n",
      "Epoch 5/5\n",
      "105/105 [==============================] - 4s 41ms/step - loss: 0.0693 - accuracy: 0.9770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1dc0079b850>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit(train_images_mlp, train_labels, epochs=5, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 997us/step - loss: 0.4118 - accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model_1.evaluate(val_images_mlp, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 32)        416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               16777728  \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 1026      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 16,779,170\n",
      "Trainable params: 16,779,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2 = models.Sequential()\n",
    "model_2.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        strides=(1,1),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(64, 64, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model_2.add(layers.MaxPooling2D(pool_size=(2,2),\n",
    "                     strides=2))\n",
    "model_2.add(layers.Flatten())        \n",
    "model_2.add(layers.Dense(512))\n",
    "model_2.add(layers.Activation('relu'))\n",
    "#model_1.add(layers.Dropout(0.25))\n",
    "model_2.add(layers.Dense(2))\n",
    "model_2.add(layers.Activation('sigmoid'))\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "105/105 [==============================] - 8s 75ms/step - loss: 0.4999 - accuracy: 0.8562\n",
      "Epoch 2/5\n",
      "105/105 [==============================] - 9s 83ms/step - loss: 0.1120 - accuracy: 0.9592\n",
      "Epoch 3/5\n",
      "105/105 [==============================] - 8s 80ms/step - loss: 0.0927 - accuracy: 0.9661\n",
      "Epoch 4/5\n",
      "105/105 [==============================] - 9s 83ms/step - loss: 0.0716 - accuracy: 0.9747\n",
      "Epoch 5/5\n",
      "105/105 [==============================] - 9s 85ms/step - loss: 0.0591 - accuracy: 0.9801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x156970ff370>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model_2.fit(train_images_mlp, train_labels, epochs=5, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 997us/step - loss: 0.1978 - accuracy: 0.9375\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model_2.evaluate(val_images_mlp, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 32)        416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4096)              134221824 \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 8194      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 134,230,434\n",
      "Trainable params: 134,230,434\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3 = models.Sequential()\n",
    "model_3.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        strides=(1,1),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(64, 64, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model_3.add(layers.MaxPooling2D(pool_size=(2,2),\n",
    "                     strides=2))\n",
    "model_3.add(layers.Flatten())        \n",
    "model_3.add(layers.Dense(4096))\n",
    "model_3.add(layers.Activation('relu'))\n",
    "#model_1.add(layers.Dropout(0.25))\n",
    "model_3.add(layers.Dense(2))\n",
    "model_3.add(layers.Activation('sigmoid'))\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "105/105 [==============================] - 53s 506ms/step - loss: 0.7669 - accuracy: 0.8838\n",
      "Epoch 2/5\n",
      "105/105 [==============================] - 50s 479ms/step - loss: 0.0900 - accuracy: 0.9682\n",
      "Epoch 3/5\n",
      "105/105 [==============================] - 49s 469ms/step - loss: 0.0702 - accuracy: 0.9757\n",
      "Epoch 4/5\n",
      "105/105 [==============================] - 50s 476ms/step - loss: 0.0650 - accuracy: 0.9749\n",
      "Epoch 5/5\n",
      "105/105 [==============================] - 50s 477ms/step - loss: 0.0367 - accuracy: 0.9885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x156984dd370>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model_3.fit(train_images_mlp, train_labels, epochs=5, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 996us/step - loss: 0.1978 - accuracy: 0.9375\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model_2.evaluate(val_images_mlp, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Pixels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_pixels = 224, 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 images belonging to 2 classes.\n",
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "        val_data_dir, \n",
    "        target_size=(more_pixels), batch_size=16)\n",
    "\n",
    "# Get all the data in the directory data/train (790 images), and reshape them\n",
    "train_generator = ImageDataGenerator().flow_from_directory(\n",
    "        train_data_dir, \n",
    "        target_size=(more_pixels), batch_size=5216)\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "        test_data_dir, \n",
    "        target_size=(more_pixels), batch_size=624)\n",
    "\n",
    "test_images, test_labels = next(test_generator)\n",
    "train_images, train_labels = next(train_generator)\n",
    "val_images, val_labels = next(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_mlp = (train_images / 255).astype('float32')\n",
    "val_images_mlp = (val_images / 255).astype('float32')\n",
    "test_images_mlp = (test_images / 255).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = models.Sequential()\n",
    "model_4.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        strides=(1,1),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(224, 224, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model_4.add(layers.MaxPooling2D(pool_size=(2,2),\n",
    "                     strides=2))\n",
    "model_4.add(layers.Flatten())        \n",
    "model_4.add(layers.Dense(128))\n",
    "model_4.add(layers.Activation('relu'))\n",
    "#model_1.add(layers.Dropout(0.25))\n",
    "model_4.add(layers.Dense(2))\n",
    "model_4.add(layers.Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 224, 224, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 401408)            0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               51380352  \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 51,381,026\n",
      "Trainable params: 51,381,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "105/105 [==============================] - 52s 494ms/step - loss: 1.1843 - recall: 0.8639\n",
      "Epoch 2/5\n",
      "105/105 [==============================] - 52s 495ms/step - loss: 0.0815 - recall: 0.9741\n",
      "Epoch 3/5\n",
      "105/105 [==============================] - 55s 528ms/step - loss: 0.0543 - recall: 0.9791\n",
      "Epoch 4/5\n",
      "105/105 [==============================] - 53s 507ms/step - loss: 0.0403 - recall: 0.9873\n",
      "Epoch 5/5\n",
      "105/105 [==============================] - 52s 493ms/step - loss: 0.0277 - recall: 0.9921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15680a4ad60>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['Recall'])\n",
    "model_4.fit(train_images_mlp, train_labels, epochs=5, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 997us/step - loss: 0.1108 - recall: 0.9375\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model_4.evaluate(val_images_mlp, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 2s 81ms/step - loss: 0.8874 - recall: 0.7885\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model_4.evaluate(test_images_mlp, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
